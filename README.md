# Bank Marketing Prediction - ML Pipeline

A production-ready machine learning pipeline for predicting bank marketing campaign success (term deposit subscription).

## üìã Project Overview

This project implements a complete ML pipeline that:
- ‚úÖ Prepares and preprocesses bank marketing data with reproducible transformations
- ‚úÖ Trains an XGBoost classifier with class imbalance handling
- ‚úÖ Evaluates model performance with comprehensive metrics (F1, ROC-AUC, etc.)
- ‚úÖ Packages the model in PyTorch-compatible format for production serving
- ‚úÖ Logs all experiments and maintains full reproducibility

## üèóÔ∏è Project Structure

```
.
‚îú‚îÄ‚îÄ config.yaml              # Complete configuration for the pipeline
‚îú‚îÄ‚îÄ preprocessing.py         # Reusable data preprocessing module
‚îú‚îÄ‚îÄ train.py                # Model training and evaluation module
‚îú‚îÄ‚îÄ package_model.py        # Model packaging for deployment
‚îú‚îÄ‚îÄ pipeline.py             # Main orchestration script
‚îú‚îÄ‚îÄ requirements.txt        # Python dependencies
‚îú‚îÄ‚îÄ README.md              # This file
‚îÇ
‚îú‚îÄ‚îÄ preprocessor.pkl        # Saved preprocessor (after running)
‚îú‚îÄ‚îÄ model.pkl              # Saved model (after running)
‚îÇ
‚îú‚îÄ‚îÄ logs/                  # Training logs and evaluation results
‚îÇ   ‚îú‚îÄ‚îÄ training_YYYYMMDD_HHMMSS.log
‚îÇ   ‚îú‚îÄ‚îÄ metrics_YYYYMMDD_HHMMSS.json
‚îÇ   ‚îú‚îÄ‚îÄ predictions_YYYYMMDD_HHMMSS.csv
‚îÇ   ‚îú‚îÄ‚îÄ confusion_matrix_YYYYMMDD_HHMMSS.png
‚îÇ   ‚îî‚îÄ‚îÄ feature_importance_YYYYMMDD_HHMMSS.png
‚îÇ
‚îî‚îÄ‚îÄ models/                # Packaged models for deployment
    ‚îî‚îÄ‚îÄ bank_marketing_model_vYYYYMMDD_HHMMSS/
        ‚îú‚îÄ‚îÄ xgboost_model.pkl
        ‚îú‚îÄ‚îÄ pytorch_model.pt
        ‚îú‚îÄ‚îÄ preprocessor.pkl
        ‚îú‚îÄ‚îÄ config.yaml
        ‚îú‚îÄ‚îÄ feature_names.json
        ‚îú‚îÄ‚îÄ metrics.json
        ‚îú‚îÄ‚îÄ metadata.json
        ‚îî‚îÄ‚îÄ README.md
```

## üöÄ Quick Start

### 1. Environment Setup

```bash
# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Run the Complete Pipeline

```bash
# Run the full pipeline
python pipeline.py --data path/to/bank-marketing.csv

# Run without model packaging (faster for experimentation)
python pipeline.py --data path/to/bank-marketing.csv --skip-packaging

# Use custom configuration
python pipeline.py --data path/to/bank-marketing.csv --config my_config.yaml
```

### 3. Check Results

After running, you'll find:
- **Model artifacts**: `model.pkl`, `preprocessor.pkl`
- **Training logs**: `logs/training_*.log`
- **Evaluation metrics**: `logs/metrics_*.json`
- **Visualizations**: `logs/confusion_matrix_*.png`, `logs/feature_importance_*.png`
- **Production package**: `models/bank_marketing_model_v*/`

## üìä Configuration

All parameters are controlled via `config.yaml`:

### Key Configuration Sections:

#### 1. **Outlier Removal**
```yaml
outlier_removal:
  iqr_multiplier: 3.0
  iqr_features:
    - age
    - balance
    - duration
    - campaign
  threshold_removals:
    previous:
      max_value: 50
    days_since_contact:
      max_value: 800
```

#### 2. **Model Parameters**
```yaml
model:
  xgboost:
    n_estimators: 100
    max_depth: 6
    learning_rate: 0.1
    scale_pos_weight: 7.5  # Handles class imbalance
```

#### 3. **Evaluation Thresholds**
```yaml
evaluation:
  primary_metric: "f1"
  f1_threshold: 0.40  # Minimum acceptable F1 score
  classification_threshold: 0.5
```

## üîß Module Usage

### Individual Module Usage

#### Preprocessing
```python
from preprocessing import BankMarketingPreprocessor, load_data

# Load data
df = load_data("path/to/data.csv")

# Preprocess
preprocessor = BankMarketingPreprocessor("config.yaml")
X_train, X_test, y_train, y_test = preprocessor.fit_transform(df)

# Save for reuse
preprocessor.save("preprocessor.pkl")

# Use on new data
X_new = preprocessor.transform(new_df)
```

#### Training
```python
from train import BankMarketingTrainer

# Train model
trainer = BankMarketingTrainer("config.yaml")
model = trainer.train(X_train, y_train)

# Evaluate
metrics = trainer.evaluate(X_test, y_test)

# Save model
trainer.save_model("model.pkl")
```

#### Packaging
```python
from package_model import ModelPackager

# Package for deployment
packager = ModelPackager("config.yaml")
package_dir = packager.package_model(
    model=model,
    preprocessor=preprocessor,
    feature_names=preprocessor.feature_names,
    metrics=metrics
)
```

## üìà Evaluation Metrics

The pipeline evaluates models using:
- **Accuracy**: Overall correctness
- **Precision**: Positive predictive value
- **Recall**: True positive rate
- **F1 Score**: Harmonic mean of precision and recall (PRIMARY METRIC)
- **ROC-AUC**: Area under ROC curve
- **PR-AUC**: Area under Precision-Recall curve

### Model Acceptance Criteria

Models must achieve:
- **F1 Score ‚â• 0.40** (configurable in `config.yaml`)

The pipeline logs whether the model is accepted or needs improvement.

## üîÑ Data Preprocessing Steps

The pipeline implements the following preprocessing:

### 1. Feature Engineering
- Remove `day` feature (not meaningful)
- Split `pdays` into:
  - `was_contacted_before`: Binary indicator
  - `days_since_contact`: Days since last contact

### 2. Outlier Removal
- Apply 3√óIQR rule for: `age`, `balance`, `duration`, `campaign`
- Remove rows with `previous` > 50
- Remove rows with `days_since_contact` > 800

### 3. Categorical Encoding
- Label encoding for all categorical features
- Keep 'unknown' as separate category

### 4. Target Encoding
- Convert 'yes'/'no' to 1/0

### 5. Train-Test Split
- 80-20 split with stratification

## üéØ Model Details

### Algorithm: XGBoost Classifier

**Why XGBoost?**
- Handles mixed numerical and categorical features well
- Robust to outliers (tree-based splits)
- Built-in class imbalance handling (`scale_pos_weight`)
- Native support for missing values
- Provides feature importance

### Class Imbalance Handling
- Dataset: 88.3% negative, 11.7% positive
- Solution: `scale_pos_weight = 7.5`
- Stratified train-test split

## üì¶ Model Packaging

Models are packaged with:

1. **XGBoost Native Format** (`xgboost_model.pkl`)
   - For direct XGBoost inference
   
2. **PyTorch Wrapper** (`pytorch_model.pt`)
   - TorchScript format for PyTorch serving
   - Compatible with PyTorch ecosystem
   
3. **Preprocessor** (`preprocessor.pkl`)
   - Fitted label encoders
   - Feature transformation logic
   
4. **Metadata** (`metadata.json`)
   - Model version
   - Performance metrics
   - Feature names
   - Training configuration

### Loading Packaged Model

```python
import pickle
import torch

# Load XGBoost model
with open('models/bank_marketing_model_v*/xgboost_model.pkl', 'rb') as f:
    model = pickle.load(f)

# Load preprocessor
with open('models/bank_marketing_model_v*/preprocessor.pkl', 'rb') as f:
    preprocessor = pickle.load(f)
```

## üîç Logging and Tracking

All training runs are logged:

### Log Files
- **Training logs**: Complete execution trace
- **Metrics**: JSON format for programmatic access
- **Predictions**: CSV with true labels, predictions, and probabilities

### Visualizations
- **Confusion Matrix**: Classification performance breakdown
- **Feature Importance**: Top 15 most important features

## üõ†Ô∏è Future Improvements

The configuration file includes notes on future improvements:
- Hyperparameter tuning (GridSearch/Optuna)
- Feature selection (SHAP, RFE)
- Ensemble methods
- SMOTE or other resampling
- Advanced feature engineering
- Alternative algorithms (LightGBM, CatBoost)
- Online learning capabilities
- Data drift detection
- A/B testing framework

## üìù Reproducibility

The pipeline ensures reproducibility through:
- ‚úÖ Fixed random seeds (set to 42)
- ‚úÖ Complete configuration tracking
- ‚úÖ Version-controlled preprocessing
- ‚úÖ Deterministic train-test splits
- ‚úÖ Saved preprocessor states

# Configuration file for Bank Marketing ML Pipeline
# This file contains all hyperparameters, thresholds, and settings for reproducible model training

# General settings
general:
  random_seed: 42
  project_name: "bank_marketing_prediction"
  
# Data settings
data:
  target_column: "y"
  test_size: 0.2
  stratify: true
  
# Feature engineering settings
feature_engineering:
  # Features to remove
  features_to_drop:
    - "day"  # Doesn't provide meaningful information in numeric form
  
  # pdays transformation
  pdays:
    never_contacted_value: -1  # Value indicating never contacted
    create_binary_feature: true  # Create was_contacted_before feature
    create_days_feature: true    # Create days_since_contact feature
    days_for_never_contacted: 0  # Value for days_since_contact when never contacted

# Outlier removal settings
outlier_removal:
  # Apply 3×IQR rule for these features
  iqr_multiplier: 3.0
  iqr_features:
    - "age"
    - "balance"
    - "duration"
    - "campaign"
  
  # Specific threshold removals
  threshold_removals:
    previous:
      max_value: 50
    days_since_contact:
      max_value: 800

# Categorical encoding settings
categorical_encoding:
  method: "label"  # Options: "label" or "onehot"
  # Keep 'unknown' as separate category for these features
  keep_unknown_features:
    - "poutcome"  # 81.75% unknown
    - "contact"   # 28.80% unknown
    - "education" # 4.11% unknown
    - "job"       # 0.64% unknown
  
  categorical_features:
    - "job"
    - "marital"
    - "education"
    - "default"
    - "housing"
    - "loan"
    - "contact"
    - "month"
    - "poutcome"
    - "was_contacted_before"

# Model settings
model:
  algorithm: "xgboost"
  
  # XGBoost hyperparameters
  xgboost:
    n_estimators: 100
    max_depth: 6
    learning_rate: 0.1
    min_child_weight: 1
    gamma: 0
    subsample: 0.8
    colsample_bytree: 0.8
    reg_alpha: 0
    reg_lambda: 1
    
    # Class imbalance handling
    scale_pos_weight: 7.5  # Ratio of negative to positive class (88.3% / 11.7% ≈ 7.5)
    
    # Other settings
    objective: "binary:logistic"
    eval_metric: "logloss"
    random_state: 42
    n_jobs: -1
    verbosity: 1
  
  # Training-specific settings (not constructor parameters)
  training:
    early_stopping_rounds: 10  # Moved here from xgboost params

# Training settings
training:
  validation_split: 0.2  # From training set
  cv_folds: 5  # For cross-validation (if needed)

# Evaluation settings
evaluation:
  # Primary metric
  primary_metric: "f1"
  
  # F1 score threshold for model acceptance
  f1_threshold: 0.40  # Minimum F1 score to consider model acceptable
  
  # Additional metrics to track
  metrics:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1"
    - "roc_auc"
    - "pr_auc"  # Precision-Recall AUC
  
  # Classification threshold for predictions
  classification_threshold: 0.5
  
  # Generate confusion matrix
  show_confusion_matrix: true
  
  # Feature importance
  show_feature_importance: true
  top_n_features: 15

# Logging settings
logging:
  log_dir: "logs"
  log_level: "INFO"
  log_format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  
  # Experiment tracking
  save_metrics: true
  save_predictions: true
  save_feature_importance: true

# Model packaging settings
packaging:
  model_dir: "models"
  model_name: "bank_marketing_model"
  
  # Files to save
  save_items:
    - "model"           # The trained model
    - "preprocessor"    # Preprocessing artifacts (encoders, etc.)
    - "config"          # This config file
    - "feature_names"   # List of feature names
    - "metrics"         # Evaluation metrics
  
  # Versioning
  version_format: "v{timestamp}"  # e.g., v20241118_143022
  
  # PyTorch wrapper settings
  pytorch_wrapper:
    enable: false  # Set to false to skip PyTorch wrapper (avoids tracing warnings)
    save_format: "torchscript"  # Options: "torchscript" or "state_dict"

# Notes for future improvements
future_improvements:
  - "Hyperparameter tuning using GridSearchCV or Optuna"
  - "Feature selection using SHAP values or recursive feature elimination"
  - "Try ensemble methods (stacking, voting)"
  - "Implement SMOTE or other resampling techniques"
  - "Add more sophisticated feature engineering (interactions, polynomials)"
  - "Experiment with other algorithms (LightGBM, CatBoost)"
  - "Implement online learning for model updates"
  - "Add data drift detection"
  - "Create A/B testing framework"
  - "Add model interpretability dashboard"
